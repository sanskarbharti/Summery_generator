{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2D88cdRobNM",
        "outputId": "b9f2b220-f9d6-43de-d924-2a6ed70c1d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://api.github.com/repos/facebookresearch/segment-anything/issues\n",
            "Number of issues: 30\n",
            "How can I use python to achieve the effect demonstrated by the demo, so that the mask of the corresponding area appears when hovering?\n",
            "Sam decoder model is returning the error : Cannot read property 'buffer' of undefined,   when i try to load the sam encoder image embeddings with .txt file extension. Using ONNX react-native runtime.\n",
            "https://segment-anything.com/demo is working fine. But my local scripts/amg.py is not working as well as https://segment-anything.com/demo\n",
            "[+] add save and load embedding methods in SamPredictor class\n",
            "Multiple coords + Multiple boxes + Multiple labels solution - Need suggest\n",
            "How to use it with text prompt\n",
            "Use a class attribute for type checking\n",
            "Got invalid dimensions for input: mask_input for the following indices\n",
            "Discrepancy results between online demo and SamAutomaticMaskGenerator\n",
            "What is the best way to get only the objects I want?\n",
            "ONNX model produces worse result than Pytorch counterpart\n",
            "return low_res logits on SamAutomaticMaskGenerator.generate()\n",
            "The selection area has obvious jagged problems\n",
            "How to extract each segment feature\n",
            "RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu) using Crop N layers > 1\n",
            "Output of labels/pseudo label\n",
            "Prompt detection \n",
            "Problems with the use of the official website sam(cut out function)\n",
            "dimension\n",
            "Grayscale image segmantic segmentation\n",
            "Is it possible to limit the number of splits?\n",
            "How can I combine SamAutomaticMaskGenerator with SamPredictor to process a picture?\n",
            "AttributeError: 'NoneType' object has no attribute 'HasField'\n",
            "How to implement demo effect\n",
            "Is there only one input shape for vit to correctly output mask？\n",
            "how to get hidden_state from every layers of ViT of sam vision encoder?\n",
            "How to sample 16 X 24 points on rectangle images\n",
            "How do I obtain the path of the current partition? I want to outline this area in the canvas\n",
            "How to deal with overlapping masks\n",
            "Detail about the 11 iteration during training\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Set your personal access token\n",
        "token = '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'\n",
        "\n",
        "# Define repository owner and name\n",
        "owner = 'facebookresearch'\n",
        "repo = 'segment-anything'\n",
        "\n",
        "def get_github_issues(owner, repo, token):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/issues\"\n",
        "    print(url)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        issues = response.json()\n",
        "        return issues\n",
        "    else:\n",
        "        print(f\"Failed to fetch issues: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "issues = get_github_issues(owner, repo, token)\n",
        "\n",
        "if issues:\n",
        "    print(f\"Number of issues: {len(issues)}\")\n",
        "    for issue in issues:\n",
        "        print(issue['title'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN4Qd-lHrQ3Q",
        "outputId": "94048578-0dda-441d-9e78-eaf4eadce341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of issue URLs:\n",
            "504\n"
          ]
        }
      ],
      "source": [
        "def get_github_issue_urls(owner, repo, token):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/issues\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\"\n",
        "    }\n",
        "\n",
        "    issue_urls = []\n",
        "\n",
        "    # Fetch issues\n",
        "    while url:\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            issues = response.json()\n",
        "            for issue in issues:\n",
        "                issue_urls.append(issue['url'])\n",
        "            # Check if there are more pages\n",
        "            if 'next' in response.links:\n",
        "                url = response.links['next']['url']\n",
        "            else:\n",
        "                url = None\n",
        "        else:\n",
        "            print(f\"Failed to fetch issues: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    return issue_urls\n",
        "\n",
        "issue_urls = get_github_issue_urls(owner, repo, token)\n",
        "\n",
        "if issue_urls:\n",
        "    print(\"List of issue URLs:\")\n",
        "    # for url in issue_urls:\n",
        "    #     print(url)\n",
        "    print(len(issue_urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqZY4I3socbh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "\n",
        "def process_issue_urls(issue_urls, token):\n",
        "    \"\"\"\n",
        "    Function to process a list of GitHub issue URLs.\n",
        "\n",
        "    Args:\n",
        "    - issue_urls (list): A list of GitHub issue URLs.\n",
        "    - token (str): GitHub personal access token.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\"\n",
        "    }\n",
        "\n",
        "    for url in issue_urls:\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Check rate limit\n",
        "        remaining = int(response.headers['X-RateLimit-Remaining'])\n",
        "        if remaining == 0:\n",
        "            reset_time = int(response.headers['X-RateLimit-Reset'])\n",
        "            sleep_time = reset_time - time.time() + 1\n",
        "            print(f\"Rate limit exceeded. Waiting for {sleep_time} seconds before retrying.\")\n",
        "            time.sleep(sleep_time)\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            issue_data = response.json()\n",
        "            title = issue_data.get('title', '')\n",
        "            if title is None:\n",
        "                title = ''\n",
        "            # Append title to the file\n",
        "            with open(\"all_titles.txt\", \"a\") as title_file:\n",
        "                title_file.write(title + '\\n')\n",
        "            body = issue_data.get('body', '')\n",
        "            if body is None:\n",
        "                body = ''\n",
        "            # Append body to the file\n",
        "            with open(\"all_bodies.txt\", \"a\") as body_file:\n",
        "                body_file.write(body + '\\n')\n",
        "            description = issue_data.get('description', '')\n",
        "            if description is None:\n",
        "                description = ''\n",
        "            # Append description to the file\n",
        "            with open(\"all_descriptions.txt\", \"a\") as description_file:\n",
        "                description_file.write(description + '\\n\\n')\n",
        "        else:\n",
        "            print(f\"Failed to fetch issue details for URL: {url}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "\n",
        "process_issue_urls(issue_urls, token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEqftYCrrMPg"
      },
      "outputs": [],
      "source": [
        "def read_text_from_files(directory):\n",
        "  \"\"\"\n",
        "  Reads text from all files within a directory.\n",
        "\n",
        "  Args:\n",
        "    directory: The directory containing the text files.\n",
        "\n",
        "  Returns:\n",
        "    A list of strings where each string is the content of a file.\n",
        "  \"\"\"\n",
        "  texts = []\n",
        "  for filename in os.listdir(directory):\n",
        "    with open(os.path.join(directory, filename), 'r') as f:\n",
        "      texts.append(f.read())\n",
        "  return texts\n",
        "\n",
        "# Read text from a directory called \"data\"\n",
        "texts = read_text_from_files(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFkEiu6jt4Zo",
        "outputId": "e21d9325-6d41-48f7-fd3c-1c43ec0c6ff4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['How can I use python to achieve the effect demonstrated by the demo, so that the mask of the corresponding area appears when hovering?\\n\\nWhen I use the demo for image segmentation:\\nDemo url: [https://segment-anything.com/demo]()\\nIt shows very good results.\\n\\nBut when I use **SamAutomaticMaskGenerator** and **SamPredictor,** I need to enter each coordinate separately to obtain the mask map of the corresponding area.But this is too inefficient.\\n\\nIs there any way to return the corresponding coordinates and mask map of the divided area, or can it only be achieved through the front end and Vue?\\n\\nThis is the best practice for image segmentation!\\n\\nSam decoder model is returning the error : Cannot read property \\'buffer\\' of undefined,   when i try to load the sam encoder image embeddings with .txt file extension. Using ONNX react-native runtime.\\n\\nI have the encoder image_embeddings in a text file in my root project directory. When i try to read the text file with encoder embeddings, the react-native can able to read the file. But if i pass the data to the decoder model its not even reading the image_embeddings from the text file. Its returning this error: **Cannot read property \\'buffer\\' of undefined**. \\n\\n\\n\\n\\nHere is my code please check it.\\n\\n\\n      const decoderModelPath = `${RNFS.TemporaryDirectoryPath}/vit_h_decoder.onnx`;\\n\\n      console.log(\\'Decoder model path is loading....\\');\\n\\n      await RNFS.downloadFile({\\n        fromUrl: Image.resolveAssetSource(\\n          require(\\'./models/vit_h_decoder.onnx\\'),\\n        ).uri,\\n        toFile: decoderModelPath,\\n      }).promise;\\n\\n      console.log(\\'Decoder model is started processing....\\');\\n\\n      const decoderSession = await ort.InferenceSession.create(\\n        \\'file://\\' + decoderModelPath,\\n      );\\n\\n      console.log(\\'Decoder model is loaded....\\');\\n\\n      const txtFile = `${RNFS.TemporaryDirectoryPath}/react-embeddings.txt`;\\n\\n      await RNFS.downloadFile({\\n        fromUrl: Image.resolveAssetSource(require(\\'./react-embeddings.txt\\'))\\n          .uri,\\n        toFile: txtFile,\\n      }).promise;\\n\\n      console.log(\\'Embeddings are loading...Please wait....\\');\\n\\n      const fileEmbeddings = await FileSystem.readFile(txtFile);\\n\\n      console.log(\\'Embeddings are going to parse....\\');\\n\\n      const parseDataFile = JSON.parse(fileEmbeddings);\\n\\n      console.log(Object.keys(parseDataFile));\\n      console.log(\\'Embeddings are Parsed successfully....\\');\\n\\n      console.log(\\'Feed is going to load....\\');\\n\\n      const feed = {\\n        image_embeddings: parseDataFile,\\n        point_coords: new ort.Tensor(\\n          new Float32Array([10, 10, 0, 0]),\\n          [1, 2, 2],\\n        ),\\n        point_labels: new ort.Tensor(new Float32Array([0, -1]), [1, 2]),\\n        mask_input: new ort.Tensor(\\n          new Float32Array(256 * 256),\\n          [1, 1, 256, 256],\\n        ),\\n        has_mask_input: new ort.Tensor(new Float32Array([0]), [1]),\\n        orig_im_size: new ort.Tensor(new Float32Array([684, 1024]), [2]),\\n      };\\n\\n      console.log(\\'Feed is loaded...\\');\\n      const finalData = await decoderSession.run(feed);\\n\\n      const filePath = `${RNFS.DocumentDirectoryPath}/example.txt`;\\n\\n      await RNFS.writeFile(\\n        filePath,\\n        JSON.stringify(finalData.masks.data),\\n        \\'utf8\\',\\n      );\\n\\n      await Share.open({\\n        title: \\'Share file\\',\\n        url: `file://${filePath}`,\\n      });\\n\\n      console.log(\\'Done with the decoder model\\');\\n\\n\\nhttps://segment-anything.com/demo is working fine. But my local scripts/amg.py is not working as well as https://segment-anything.com/demo\\n\\nDear all, \\n\\nI am really happy to know this amazing library is existing and you guys created it. Much appreciate. \\nMy case is strange. I am going to try to explain my issue on below:\\n\\n1. This is my original image that I wanted to segment:\\n<img width=\"998\" alt=\"original-image\" src=\"https://github.com/facebookresearch/segment-anything/assets/37650799/41731f85-ab6b-49a0-9cea-9e7ac96330f0\">\\n\\n2. My goal is to segment polygon shape where is placed at the center. I use Macbook M1 chip. So, CUDA is not supported in my case.\\n\\n`python scripts/amg.py --device cpu --checkpoint ../checkpoints/sam_vit_h_4b8939.pth --model-type vit_h --output metadata --input original-image.png`\\n\\nHere is my metadata: [\\n[metadata.zip](https://github.com/facebookresearch/segment-anything/files/14883186/metadata.zip)\\n]\\n\\nI got below shape at least. Because it is very differenciatable than other shapes. But this one is not what I want to get.\\n![2](https://github.com/facebookresearch/segment-anything/assets/37650799/d94a7ef5-fc57-41e0-979a-18bbc0c8b586)\\n\\nThis is what I want to get from the image:\\n<img width=\"998\" alt=\"original-image-to-achieve\" src=\"https://github.com/facebookresearch/segment-anything/assets/37650799/04d660db-144f-41e6-a385-cd0de1976887\">\\n\\n3. When I upload this original-image.png to https://segment-anything.com/demo# and selected \"Everything\", seems that \"Everything\" option is working fine: \\n<img width=\"1547\" alt=\"Screenshot 2024-04-05 at 17 19 30\" src=\"https://github.com/facebookresearch/segment-anything/assets/37650799/cf5fb974-f9db-4b0b-a135-d1c009ae9ecd\">\\n\\nHow I can work segment-anything as https://segment-anything.com/demo# with \"Everything\" option on my local environment ?\\n\\nDo you have any suggestions in my case ?\\n\\nThanks in advance!\\nMuch appreciate any help.\\n\\n\\n\\n[+] add save and load embedding methods in SamPredictor class\\n\\nAdded save and load functionality for image embeddings in SamPredictor class.\\n\\nThis pull request includes two new methods in the SamPredictor class:\\n\\n- `save_image_embedding(self, path)`: This method allows saving the image embedding to a specified path. It raises a RuntimeError if an image has not been set with `.set_image()` before saving the embedding.\\n- `load_image_embedding(self, path)`: This method allows loading the image embedding from a specified path and sets it as an attribute of the object. It uses the `torch` library to load the saved embedding.\\n\\n\\n\\nMultiple coords + Multiple boxes + Multiple labels solution - Need suggest\\n\\ni implemented in my code \\n\\n```predictor = SamPredictor(self.sam)\\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\npredictor.set_image(image)\\n            \\ninput_boxes = np.array(boxes) if len(boxes) > 0 else None\\ninput_boxes = torch.tensor(input_boxes, device=predictor.device)\\ntransformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\\n               \\nmasks, scores, logits = predictor.predict_torch(\\n   point_coords=None,\\n   point_labels=None,\\n   boxes=transformed_boxes,\\n   multimask_output=False,\\n)\\n```\\n\\nhow can i use multiple coord and labels ? What format it is ?\\nThanks for help.\\n\\nHow to use it with text prompt\\n\\nHow to use it with text prompt? I noticed that there is only point and box prompt in the code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GwLG0git7XO",
        "outputId": "f59ac2d7-2bee-4b44-fce2-a6dbc2ae46f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L01ioqCMuoXE"
      },
      "outputs": [],
      "source": [
        "# Define the chunk size\n",
        "chunk_size = 1000\n",
        "data = '/content/all_descriptions.txt'\n",
        "# Split the data into chunks\n",
        "chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvDuEDQVurTc",
        "outputId": "da5fac1d-8928-4392-de16-d1c54b7495f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 142, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
          ]
        }
      ],
      "source": [
        "summaries = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    # Use the summarizer to summarize the chunk\n",
        "    summary = summarizer(chunk)\n",
        "\n",
        "    # Extract the summary text\n",
        "    summaries.append(summary[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJQQNzD2uw-D"
      },
      "outputs": [],
      "source": [
        "# Join the summaries into a single string\n",
        "final_summary = \"\\n\".join(summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkzIx0UHuyPM",
        "outputId": "123c3175-c2a7-4336-8573-4c9844a82b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " /content/all_descriptions.txt is a collection of text files . The text file is used to describe the contents of the book . The book is published in the U.S. State of the Art of the World . It is published under the heading of the Book of The World .\n"
          ]
        }
      ],
      "source": [
        "print(summaries[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Xo1HtJg26O",
        "outputId": "0a401ed1-4b36-4301-e8d0-d4cd1c632512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response stopped\n",
            "Certainly! Let’s talk about Segment Anything Model (SAM). It’s an exciting innovation in the field of computer vision. Here are the key points:\n",
            "\n",
            "What is SAM?\n",
            "Segment Anything Model (SAM) is a new AI model developed by Meta AI. Its standout feature is its ability to “cut out” any object in an image with a single click.\n",
            "SAM is a promptable segmentation system that doesn’t require extensive training with labeled data. This means it can segment objects without the need for additional training, even for unfamiliar objects and images1.\n",
            "Zero-Shot Generalization:\n",
            "SAM has learned a general notion of what objects are. This understanding enables zero-shot generalization to unfamiliar objects and images without requiring additional training.\n",
            "In other words, SAM can segment objects it hasn’t seen before, making it highly versatile1.\n",
            "How SAM Works:\n",
            "SAM takes input prompts specifying what to segment in an image. These prompts allow for a wide range of segmentation tasks without additional training.\n",
            "You can prompt SAM with interactive points, boxes, or even automatically segment everything in an image.\n",
            "It generates multiple valid masks for ambiguous prompts, making it robust and flexible1.\n",
            "Applications and Outputs:\n",
            "SAM’s output masks can be used in various ways:\n",
            "Tracked in videos\n",
            "Enable image editing applications\n",
            "Lifted to 3D\n",
            "Used for creative tasks like collaging1.\n",
            "Training and Data Engine:\n",
            "SAM’s advanced capabilities come from training on millions of images and masks.\n",
            "Researchers used SAM and its data to interactively annotate images and update the model. This cycle was repeated to improve both the model and the dataset.\n",
            "The final dataset includes more than 1.1 billion segmentation masks collected from ~11 million licensed and privacy-preserving images1.\n",
            "Efficiency and Flexibility:\n",
            "SAM’s promptable design allows for flexible integration with other systems.\n",
            "It’s efficient and groundbreaking in its ability to perform complex image segmentation tasks with unprecedented precision2.\n",
            "If you’d like to explore SAM further, you can try the demo or read more about it in this Medium article3. Image segmentation is a fascinating field, and SAM is pushing the boundaries! 🌟🔍🖼️4\n"
          ]
        }
      ],
      "source": [
        "print(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cs958TtVfWNJ"
      },
      "outputs": [],
      "source": [
        "# Open a new text file for writing\n",
        "with open(\"new_Readme.md\", \"w\") as f:\n",
        "    # Write some text to the file\n",
        "    f.write(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhnchOpNgtMD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
